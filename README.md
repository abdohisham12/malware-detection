# üõ°Ô∏è Malware Detection Project

## Overview
The Malware Detection Project aims to predict the likelihood of a Windows machine getting infected by various families of malware. Using telemetry data from Windows Defender, which includes machine configurations, operating system details, and antivirus product states, the project utilizes a stacked model approach to enhance detection accuracy with model accuracy of 0.6698 where the highest model accuracy in kaggle was 0.7114 .

### Stacked Model Approach
The project combines the following machine learning models:
- **Random Forest**: Aggregates predictions from multiple decision trees for improved accuracy and reduced overfitting.
- **XGBoost**: Employs gradient boosting to handle outliers and large datasets efficiently.
- **LightGBM**: Known for its speed and efficiency, using leaf-wise tree growth to improve performance on large-scale data.

## Table of Contents
- [Overview](#overview)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Model Details](#model-details)
- [Contributing](#contributing)
- [Acknowledgments](#acknowledgments)

## Installation
To set up the project locally, follow these steps:

### Step 1: Clone the repository

```bash
git clone https://github.com/yourusername/Malware-Detection.git
```
### Step 2 : Set up the environment

Create a Conda environment and install the required dependencies:
```bash
conda create --name malware_detection python=3.11.8     
conda activate malware_detection   
pip install pandas numpy scikit-learn lightgbm xgboost category-encoders statsmodels matplotlib seaborn   
```
## Usage

To run the project and use the provided notebooks:

1. Navigate to the repository directory: `cd Malware-Detection`
2. Start Jupyter Notebook: `jupyter notebook`
3. Download the dataset: You can obtain the dataset from the Microsoft Malware Prediction Competition on Kaggle: [here](https://www.kaggle.com/competitions/microsoft-malware-prediction/data).
   - Alternatively, you can use this direct link to download the dataset: [[link to the dataset file](https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/10683/220065/bundle/archive.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1726326045&Signature=NAppNQjKNskUDDXPhVWufJrR%2BvRl4WAsasHA2XxVTLrrmGNFH4wTvwZDOrHdb2Sy%2BOA%2F%2BLOzlQeLvln4igi3fM4ka9aZr4KzdL3SKSjxmB3a%2FPyHmykq4X3FrxaCOGEzm9sx6YYUCwfvO5iy8fSjY5mZGIov7pBmNvcKhRkdGXy9vO2WK17GD3t%2BEbWiZbv2UFyyIiAOsrTHCW2%2FI8EZvV4Thbsis%2FhshoHG2Fo2v2kAvE8QwT%2BEwxwbXL79yxIlSVGvSJEelyL4upwmjz5Xz0GsX8WJpvxS7EM6IAAUSpEICwyeveH6R45nWenjc7OOLCUf1QdkULAXqrqA6r2sSQ%3D%3D&response-content-disposition=attachment%3B+filename%3Dmicrosoft-malware-prediction.zip)]
4. Open the notebooks: In Jupyter Notebook, open the provided `.ipynb` files and execute the cells to run the code. Note that preprocessing or dataset formatting might be required before running the notebooks.    

## Project Structure

Malware-Detection/

‚îú‚îÄ‚îÄ data/                                  # Dataset files.   
‚îú‚îÄ‚îÄ models/                                # Saved trained models.   
‚îú‚îÄ‚îÄ stacked-model-on-all-rows.ipynb        # Main notebook .      
‚îî‚îÄ‚îÄ  README.md                             # Project documentation.   

## Model Details
The project uses a stacked model approach with the following components:
### models
Random Forest: Provides diversity in predictions by aggregating multiple decision trees.   
XGBoost: Improves performance on large datasets and handles outliers effectively.   
LightGBM: Offers high speed and efficient handling of large-scale data through leaf-wise growth. 
Stacking model : used logisitic regression model to collect output of all 3 models and push it in one model 

abdulrahman tried different classification model including 
SVC which took 4 hour to run on a 100k row sample of the data.    
KNN where best K was determined by GridSearchCV (best k was 30 ) and raised rocauc score of 0.62.   
logisitic regression raised rocauc score of 0.6153.    
### hyperparameters tuning
random forest hyperparameters is made on MLflow.   
before hyperparameter tuning   ![rf roc score ](https://github.com/user-attachments/assets/545853ce-a157-4846-85a6-d8bcef199159)

after ![random forest](https://github.com/user-attachments/assets/6d6063b3-3a44-4593-915d-cf224150beaa)

 XGboost is also made on MLflow 
 ![xgboost](https://github.com/user-attachments/assets/31e162ee-05eb-415a-9297-3aa89068b614)

LightGBM is determined by GridSearchCV ran on a 100k row sample of the data

### Evaluation Metric
ROC-AUC is used as the evaluation metric to assess model performance, particularly suitable for imbalanced datasets and the needed metric score to sumbit in kaggle competition .

## Contributing
Abdulrahman weclomes Contributions, To contribute, please fork the repository and submit a pull request with your proposed changes.


## Acknowledgments
This project is based on the Microsoft Malware Prediction Competition on Kaggle. Special thanks to the following tools and libraries used:

Pandas.   
NumPy.   
Scikit-learn.   
LightGBM.   
XGBoost.   
Category Encoders.   
Statsmodels.   
Matplotlib.   
Seaborn.   
